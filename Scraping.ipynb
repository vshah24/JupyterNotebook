{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed=urlopen('https://www.indeed.com/q-data-scientist-l-New-York-jobs.html')\n",
    "\n",
    "#using Request\n",
    "indeed_req=requests.get('https://www.indeed.com/q-data-scientist-l-New-York-jobs.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_req=BeautifulSoup(indeed,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(indeed_req.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liberty Lending\n",
      "Qloo\n",
      "05 Ascensia Diabetes Care US Inc.\n",
      "Bloomberg\n",
      "Spotify\n",
      "Raindrop\n",
      "Biz2Credit Inc.\n",
      "Better\n",
      "Capgemini\n",
      "1010data\n",
      "SupplyHouse.com\n",
      "WeWork\n",
      "Jackson Lewis P.C\n",
      "Bank of America\n",
      "Chembio Diagnostic Systems, Inc.\n",
      "Praxair\n"
     ]
    }
   ],
   "source": [
    "'''Getting the Company Name'''\n",
    "i=0\n",
    "spans = soup.findAll('span', attrs={'class': 'company'})\n",
    "spans\n",
    "for span in spans:\n",
    "    print(span.text.strip())                #Strip helps to eliminate spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York, NY',\n",
       " 'New York, NY',\n",
       " 'Valhalla, NY',\n",
       " 'New York, NY 10007 (Financial District area)',\n",
       " 'New York, NY 10011 (Chelsea area)',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'New York, NY 10018 (Clinton area)',\n",
       " 'New York, NY',\n",
       " 'New York, NY',\n",
       " 'Melville, NY',\n",
       " 'New York, NY',\n",
       " 'Melville, NY',\n",
       " 'New York, NY 10020 (Midtown area)',\n",
       " 'Medford, NY 11763',\n",
       " 'Tonawanda, NY 14150']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=0\n",
    "x=[]\n",
    "for i in soup.findAll('div',attrs={'class':'row'}):\n",
    "    \n",
    "    divs = i.findAll('div', attrs={'class': 'location'})\n",
    "    if len(divs)>0:\n",
    "        for j in divs:\n",
    "            x.append(j.text.strip())\n",
    "    else:\n",
    "        spans = i.findAll('span', attrs={'class': 'location'})\n",
    "        for k in spans:\n",
    "            x.append(k.text.strip())\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New York, NY', 'New York, NY', 'Valhalla, NY', 'New York, NY 10007 (Financial District area)', 'New York, NY 10011 (Chelsea area)', 'New York, NY', 'New York, NY', 'New York, NY 10018 (Clinton area)', 'New York, NY', 'New York, NY', 'Melville, NY', 'New York, NY', 'Melville, NY', 'New York, NY 10020 (Midtown area)', 'Medford, NY 11763', 'Tonawanda, NY 14150']\n",
      "['New York, NY', 'New York, NY', 'Valhalla, NY', 'New York, NY 10007 (Financial District area)', 'Medford, NY 11763', 'Tonawanda, NY 14150', 'New York, NY 10011 (Chelsea area)', 'New York, NY', 'New York, NY', 'New York, NY 10018 (Clinton area)', 'New York, NY', 'New York, NY', 'Melville, NY', 'New York, NY', 'Melville, NY', 'New York, NY 10020 (Midtown area)']\n"
     ]
    }
   ],
   "source": [
    "'''Location are either inside te span or div '''\n",
    "a=[]\n",
    "location=soup.findAll(['div','span'],attrs={'class':'location'})\n",
    "for i in location:\n",
    "    a.append(i.text)\n",
    "print(a)\n",
    "'''The above cell is also the same but this is much easier.'''\n",
    "'''The above method is the correct method according to me but validate it'''\n",
    "\n",
    "\n",
    "x=[]\n",
    "divs = soup.findAll('div', attrs={'class': 'location'})\n",
    "\n",
    "for div in divs:\n",
    "    x.append(div.text.strip())\n",
    "spans = soup.findAll('span', attrs={'class': 'location'})\n",
    "for span in spans:\n",
    "    x.append(span.text.strip())\n",
    "print(x)\n",
    "\n",
    "'''Check the difference in their order and it is very important when we are trying to show the data in table format'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "AI Research Scientist\n",
      "Associate Data Scientist, Premium Analytics\n",
      "Data Scientist Intern\n",
      "Data Scientist\n",
      "Data Scientist- Better\n",
      "Data Scientist\n",
      "Data Scientist, Intern (2019)\n",
      "Data Scientist\n",
      "Data Scientist Summer Intern\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "R&D Scientist, Jr.\n",
      "Data Scientist\n"
     ]
    }
   ],
   "source": [
    "'''Positon'''\n",
    "position=soup.findAll('a',attrs={'data-tn-element':'jobTitle'})\n",
    "for i in position:\n",
    "        print(i['title'])     #We are getting the title attribute inside tag a *****\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "'''Just to verify the code for the below cell*****\n",
    "z=soup.findAll('span',attrs={'class':'date'})\n",
    "for i in z:\n",
    "    if (i['class'])==['date']:\n",
    "        print(1)\n",
    "    else:\n",
    "        print(0)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing\n",
      "nothing\n",
      "nothing\n",
      "nothing\n",
      "9 days ago\n",
      "13 days ago\n",
      "30+ days ago\n",
      "20 hours ago\n",
      "12 hours ago\n",
      "30+ days ago\n",
      "2 days ago\n",
      "8 days ago\n",
      "1 day ago\n",
      "7 days ago\n",
      "nothing\n",
      "nothing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"We observe that not all have the Posted Time.So we will have to take care when we don't have any values\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Multiple tags and attributes inside findAll function'''\n",
    "Posted_Time=soup.findAll(['span','div'],attrs={'class':['date','sjCapt']})\n",
    "for i in Posted_Time:\n",
    "        if i['class']==['date']:\n",
    "            print(i.text)\n",
    "        else:\n",
    "            print('nothing')\n",
    "        \n",
    "'''We observe that not all have the Posted Time.So we will have to take care when we don't have any values'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Lets Merge everything'''\n",
    "\n",
    "#Company Name\n",
    "company_name=[]\n",
    "company= soup.findAll('span', attrs={'class': 'company'})\n",
    "for h in company:\n",
    "    company_name.append(h.text.strip())\n",
    "\n",
    "\n",
    "\n",
    "#LOCATION\n",
    "location=[]\n",
    "locations=soup.findAll(['div','span'],attrs={'class':'location'})\n",
    "for i in locations:\n",
    "    location.append(i.text.strip())\n",
    "    \n",
    "#Positon\n",
    "position=[]\n",
    "positions=soup.findAll('a',attrs={'data-tn-element':'jobTitle'})\n",
    "for j in positions:\n",
    "        position.append(j.text.strip()) \n",
    "        \n",
    "\n",
    "        \n",
    "#Days Posted\n",
    "posted=[]\n",
    "Posted_Time=soup.findAll(['span','div'],attrs={'class':['date','sjCapt']})\n",
    "for k in Posted_Time:\n",
    "        if k['class']==['date']:\n",
    "            posted.append(k.text.strip())\n",
    "        else:\n",
    "            posted.append('Not Mentioned')\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df=pd.DataFrame(data={'Company':company_name,'Position':position,'Location':location,'Job_Posted':posted})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Position</th>\n",
       "      <th>Location</th>\n",
       "      <th>Job_Posted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liberty Lending</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Not Mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qloo</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Not Mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05 Ascensia Diabetes Care US Inc.</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Valhalla, NY</td>\n",
       "      <td>Not Mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>AI Research Scientist</td>\n",
       "      <td>New York, NY 10007 (Financial District area)</td>\n",
       "      <td>Not Mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spotify</td>\n",
       "      <td>Associate Data Scientist, Premium Analytics</td>\n",
       "      <td>New York, NY 10011 (Chelsea area)</td>\n",
       "      <td>9 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Raindrop</td>\n",
       "      <td>Data Scientist Intern</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>13 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Biz2Credit Inc.</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>30+ days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Better</td>\n",
       "      <td>Data Scientist- Better</td>\n",
       "      <td>New York, NY 10018 (Clinton area)</td>\n",
       "      <td>20 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Capgemini</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>12 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1010data</td>\n",
       "      <td>Data Scientist, Intern (2019)</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>30+ days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SupplyHouse.com</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Melville, NY</td>\n",
       "      <td>2 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WeWork</td>\n",
       "      <td>Data Scientist Summer Intern</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>8 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Jackson Lewis P.C</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Melville, NY</td>\n",
       "      <td>1 day ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bank of America</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY 10020 (Midtown area)</td>\n",
       "      <td>7 days ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chembio Diagnostic Systems, Inc.</td>\n",
       "      <td>R&amp;D Scientist, Jr.</td>\n",
       "      <td>Medford, NY 11763</td>\n",
       "      <td>Not Mentioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Praxair</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Tonawanda, NY 14150</td>\n",
       "      <td>Not Mentioned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Company  \\\n",
       "0                     Liberty Lending   \n",
       "1                                Qloo   \n",
       "2   05 Ascensia Diabetes Care US Inc.   \n",
       "3                           Bloomberg   \n",
       "4                             Spotify   \n",
       "5                            Raindrop   \n",
       "6                     Biz2Credit Inc.   \n",
       "7                              Better   \n",
       "8                           Capgemini   \n",
       "9                            1010data   \n",
       "10                    SupplyHouse.com   \n",
       "11                             WeWork   \n",
       "12                  Jackson Lewis P.C   \n",
       "13                    Bank of America   \n",
       "14   Chembio Diagnostic Systems, Inc.   \n",
       "15                            Praxair   \n",
       "\n",
       "                                       Position  \\\n",
       "0                                Data Scientist   \n",
       "1                                Data Scientist   \n",
       "2                                Data Scientist   \n",
       "3                         AI Research Scientist   \n",
       "4   Associate Data Scientist, Premium Analytics   \n",
       "5                         Data Scientist Intern   \n",
       "6                                Data Scientist   \n",
       "7                        Data Scientist- Better   \n",
       "8                                Data Scientist   \n",
       "9                 Data Scientist, Intern (2019)   \n",
       "10                               Data Scientist   \n",
       "11                 Data Scientist Summer Intern   \n",
       "12                               Data Scientist   \n",
       "13                               Data Scientist   \n",
       "14                           R&D Scientist, Jr.   \n",
       "15                               Data Scientist   \n",
       "\n",
       "                                        Location     Job_Posted  \n",
       "0                                   New York, NY  Not Mentioned  \n",
       "1                                   New York, NY  Not Mentioned  \n",
       "2                                   Valhalla, NY  Not Mentioned  \n",
       "3   New York, NY 10007 (Financial District area)  Not Mentioned  \n",
       "4              New York, NY 10011 (Chelsea area)     9 days ago  \n",
       "5                                   New York, NY    13 days ago  \n",
       "6                                   New York, NY   30+ days ago  \n",
       "7              New York, NY 10018 (Clinton area)   20 hours ago  \n",
       "8                                   New York, NY   12 hours ago  \n",
       "9                                   New York, NY   30+ days ago  \n",
       "10                                  Melville, NY     2 days ago  \n",
       "11                                  New York, NY     8 days ago  \n",
       "12                                  Melville, NY      1 day ago  \n",
       "13             New York, NY 10020 (Midtown area)     7 days ago  \n",
       "14                             Medford, NY 11763  Not Mentioned  \n",
       "15                           Tonawanda, NY 14150  Not Mentioned  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
